{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tarea 4 - HMM, MEMM, CRF, CNN, RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "# **Tarea 4 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes: Louise Schmidt - Sebasti√°n Urbina**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** Martes 22 de junio.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n: 5 horas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6jB5fLGMCaI"
      },
      "source": [
        "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n",
        "En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n",
        "\n",
        "Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Axiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s) les puede ser de *gran ayuda*.\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
        "\n",
        "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "- [MEMMs and CRFs](slides/NLP-CRF.pdf) | ([tex source file](slides/NLP-CRF.tex)), [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
        "- [Convolutional Neural Networks](slides/NLP-CNN.pdf) | ([tex source file](slides/NLP-CNN.tex)), [video](https://youtu.be/lLZW5Fn40r8)\n",
        "- [Recurrent Neural Networks](slides/NLP-RNN.pdf) | ([tex source file](slides/NLP-RNN.tex)), [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXD3D7RYKJ-"
      },
      "source": [
        "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n",
        "\n",
        "### Pregunta 1 (1 pt)\n",
        "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
        "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
        "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "e(the|\\text{ DET}) &= 0.5 \\\\\n",
        "e(pasta|\\text{ NOUN}) &= 0.6\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
        "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
        "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
        "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UvNmJMvi83q"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "\n",
        "\\begin{array}$\n",
        "    \\pi(8,\\text{DET},\\text{NOUN}) &= max_{w\\in S_{6}}(\\pi(7,w,DET)\\cdot q(NOUN|w,DET)\\cdot e(pasta|NOUN) \\\\\n",
        "    &= \\pi(7,VERB,DET)\\cdot q(NOUN|VERB,DET)\\cdot e(pasta|NOUN) \\\\ \n",
        "    &= 0.01\\cdot 0.3 \\cdot 0.6 \\\\\n",
        "    &= 0.0018\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwJb_vmkKLZ"
      },
      "source": [
        "### Pregunta 2 (0.5 pts)\n",
        "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
        "\n",
        "#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n",
        "\n",
        "**Respuesta:** Sirven para tareas del tipo Secuence Labbeling/Tagging. Un ejemplo es Name Entity Recognition. Esta tarea consiste en determinar a qu√© entidad corresponde cada palabra, si corresponde a una empresa, un lugar, una persona o no entidad en el caso de que no lo sea. El output es una secuencia de etiquetas del mismo tama√±o que la cantidad de palabras. Para definir las entidades se consideran las palabras anteriores, por ejemplo, se puede tener inicio de compa√±√≠a y luego continuaci√≥n de compa√±√≠a.\n",
        "\n",
        "Otra tarea es Part-of-Speech Tagging. El output corresponde a una secuencia de etiquetas de tipo Noun, Verb, Preposition, Adverb, Adjective, etc. Dado determinado idioma se tienen ciertas secuencias de estas etiquetas, por ejemplo, un art√≠culo puede anteceder un sustantivo, un sustantivo puede anteceder un adjetivo, etc. De esta manera, se buscan las combinaciones posibles de estas etiquetas y a partir de estas se determina cual es la m√°s probable. \n",
        "\n",
        "#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n",
        "\n",
        "**Respuesta:** El modelo MEMM usa features en Part-of-Speech Tagging. Es √∫til para poder codificar un evento, por ejemplo, el evento de que una palabra sea un adverbio y temine en -ly (wrongly, loudly). A trav√©s de esto, se le puede dar mayor peso al vector de peso para determinar el tipo de palabra y un menor peso para casos con baja probabilidad. Se utilizan templates de features para no tener que programar estos features a mano, pues pueden ser much√≠simos.\n",
        "\n",
        "El modelo CRF tambi√©n ocupa un feature vector global que mapea toda la secuencia de entrada $x_{1:m}$ con una secuencia de tags $s_{1:m}$ a un vector d-dimensional $\\vec{\\Phi}(x_{1:m},s_{1:m}) \\in R^{d}$.\n",
        "\n",
        "#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n",
        "\n",
        "**Respuesta:** El modelo HMM lo maneja reemplazando las palabras poco frecuentes por ciertas categor√≠as. Se puede dividir el vocabulario en dos conjuntos distintos, uno de palabras frecuentes (al menos 5 veces en el cuerpo) y otro de palabras poco frecuentes. Despu√©s, las pertenecientes a poco frecuente se mapean seg√∫n sus prefijos y sufijos a un conjunto fijo de categor√≠a.\n",
        "\n",
        "En cuanto a los modelos MEMM y CRF, como estos optimizan los pesos para cada palabra en vez de realizar un conteo como en HMM, las palabras con baja frecuencia no afectan dr√°sticamente el desempe√±o. \n",
        "\n",
        "#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n",
        "\n",
        "**Respuesta:** El modelo CRF es capaz de tomar decisiones globales ya que toma toda la secuencia de etiquetas en cuenta y no est√° restringida a solo una posici√≥n de la etiqueta como lo est√°n en MEMM y HMM. Trabaja con normalizaci√≥n global.\n",
        "\n",
        "Los MEMMs y HMM tienen una memoria reducida al momento de etiquetar las palabras. En comparaci√≥n al modelo HMM, MEMM es capaz de **observar** la secuencia completa y no s√≥lo la etiqueta misma de cada palabra. Trabaja con normalizaci√≥n local.\n",
        "\n",
        "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n",
        "\n",
        "**Respuesta:** Dada esta secuencia, si se tienen k etiquetas posibles la cantidad de secuencias de etiquetas es de $k^{m}$. Como se tienen probabilidades condicionales, se hace un supuesto de independencia para que no sea caro de modelar. En el caso de MEMM se hace un Markoviano de primer orden, se observa s√≥lo la etiqueta anterior. Con el supuesto de independencia se modela cada t√©rmino usando multiclass log-linear (softmax).\n",
        "\n",
        "Tanto para HMM como MeMM se tiene el algoritmo de Viterbi, el cual permite, a trav√©s de programaci√≥n din√°mica, encontrar la mejor secuencia. En este caso se tiene un tiempo lineal y cuadr√°tico $O(mk^{2})$ y es computable ya que se utilizan valores anteriores (primer o segundo orden de Markov).\n",
        "\n",
        "Por otro lado, el modelamiento de CRF es m√°s caro pues se hace directamente en la probabilidad de la secuencia de etiquetas dado la secuencia de palabras, la normalizaci√≥n se hace sobre todas las etiquetas posibles. Por lo tanto, se toman ciertos supuestos sobre $\\Phi$ tal que sea posible entrenar y codificar el modelo. En la suma del denominador se utiliza programaci√≥n din√°mica para manejar la gran cantidad de datos a trav√©s de sumas locales mirando s√≥lo dos etiquetas con tiempo polinomial (Forward-backward algorithm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRAHR95Y8aB"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "### Pregunta 3 (1 pt)\n",
        "\n",
        "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
        "\n",
        "La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n",
        "\\begin{equation}\n",
        "E = \\begin{pmatrix}\n",
        "2 & 2\\\\\n",
        "0 & -2\\\\\n",
        "0 & 1\\\\\n",
        "-2 & 1\\\\\n",
        "1 & 0\\\\\n",
        "-1 & 1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Los siguientes 3 filtros\n",
        "\\begin{equation}\n",
        "U = \\begin{pmatrix}\n",
        "-1 & 1 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 0 & -1\\\\\n",
        "1 & -1 & -1\\\\\n",
        "-1 & -1 & 1\\\\\n",
        "1 & 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Y la funci√≥n de activaci√≥n\n",
        "\\begin{equation}\n",
        "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
        "\\end{equation}\n",
        "\n",
        "Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQ30Arkq0u4"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "El tama√±o de la ventana debe ser $k = 3$(3-gramas). Ya que la matriz de filtros tiene 6 filas, por tanto se debe cumplir que $d_{emb}\\cdot k = 2\\cdot k = 6$\n",
        "\n",
        "Sea $E_{[W_i]}=\\vec{W_i}$, los embedding de cada palabra $W_i$.\n",
        "\n",
        "Se tienen las siguientes secuencias por cada 3-grama en la oraci√≥n:\n",
        "\\begin{array}$\n",
        "    \\vec{x_1} &= [\\vec{W_1};\\vec{W_2};\\vec{W_3}] = [2,2;0,-2;0,1] \\\\\n",
        "    \\vec{x_2} &= [\\vec{W_2};\\vec{W_3};\\vec{W_4}] = [0,-2;0,1;-2,1] \\\\\n",
        "    \\vec{x_3} &= [\\vec{W_3};\\vec{W_4};\\vec{W_5}] = [0,1;-2,1;1,0] \\\\\n",
        "    \\vec{x_4} &= [\\vec{W_4};\\vec{W_5};\\vec{W_6}] = [-2,1;1,0;-1,1] \\\\\n",
        "    \\vec{x_5} &= [\\vec{W_5};\\vec{W_6};\\vec{W_7}] = [1,0;-1,1;1,1] \n",
        "\\end{array}\n",
        "Asi, de forma matricial se tiene:\n",
        "\\begin{equation}\n",
        "X = \\begin{pmatrix}\n",
        "    2 & 2 & 0 & -2 & 0 & 1 \\\\\n",
        "    0 & -2 & 0 & 1 & -2 & 1 \\\\\n",
        "    0 & 1 & -2 & 1 & 1 & 0 \\\\\n",
        "    -2 & -2 & 1 & 0 & -1 & 1 \\\\\n",
        "    1 & 0 & -1 & 1 & 1 & 1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "- $X \\in R^{|3-gramas|x(k\\cdot d_{emb})}$\n",
        "\n",
        "Luego, aplicando el filtro.\n",
        "\\begin{equation}\n",
        "    P = tanh(X\\cdot U) = tanh\\begin{pmatrix}\n",
        "-1 & 6 & 1\\\\\n",
        "2 & -1 & -4\\\\\n",
        "1 & -1 & 2\\\\\n",
        "2 & -3 & -3\\\\\n",
        "0 & -1 & 0\n",
        "\\end{pmatrix} = \n",
        "\\begin{pmatrix}\n",
        "\\frac{e^{2(-1)} - 1}{e^{2(-1)} + 1} & \\frac{e^{2(6)} - 1}{e^{2(6)} + 1} & \\frac{e^{2(1)} - 1}{e^{2(1)} + 1} \\\\\n",
        "\\frac{e^{2(2)} - 1}{e^{2(2)} + 1} & \\frac{e^{2(-1)} - 1}{e^{2(-1)} + 1} & \\frac{e^{2(-4)} - 1}{e^{2(-4)} + 1} \\\\\n",
        "\\frac{e^{2(1)} - 1}{e^{2(1)} + 1} & \\frac{e^{2(-1)} - 1}{e^{2(-1)} + 1} & \\frac{e^{2(2)} - 1}{e^{2(2)} + 1} \\\\\n",
        "\\frac{e^{2(2)} - 1}{e^{2(2)} + 1} & \\frac{e^{2(-3)} - 1}{e^{2(-3)} + 1} & \\frac{e^{2(-3)} - 1}{e^{2(-3)} + 1} \\\\\n",
        "\\frac{e^{2(0)} - 1}{e^{2(0)} + 1} & \\frac{e^{2(-1)} - 1}{e^{2(-1)} + 1} & \\frac{e^{2(0)} - 1}{e^{2(0)} + 1} \n",
        "\\end{pmatrix} = \n",
        "\\begin{pmatrix}\n",
        "-0.761 & 0.999  & 0.761\\\\\n",
        "0.964  & -0.761 & -0.999\\\\\n",
        "0.761  & -0.761 & 0.964\\\\\n",
        "0.964  & -0.995 & -0.995\\\\\n",
        "0  & -0.761 & 0\n",
        "\\end{pmatrix} \n",
        "\\end{equation}\n",
        "Donde,\n",
        "\\begin{equation}\n",
        "    \\vec{p_i} = tanh(\\vec{x_i}\\cdot U)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Recordar que en este caso tenemos $m=5$(3-gramas totales) y $l=3$ filtros.\n",
        "Finalmente aplicando max pooling se tiene:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{c_{[j]}} = max_{1\\leq i\\leq m=5}\\vec{p_i}_{[j]} \\quad \\forall j\\in[1,l=3]\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\vec{c} = \\left[max\\begin{pmatrix}\n",
        "-0.761 \\\\\n",
        "0.964  \\\\\n",
        "0.761  \\\\\n",
        "0.964 \\\\\n",
        "0  \n",
        "\\end{pmatrix}, \n",
        "max\\begin{pmatrix}\n",
        "0.999  \\\\\n",
        "-0.761 \\\\\n",
        "-0.761 \\\\\n",
        "-0.995 \\\\\n",
        "-0.761 \n",
        "\\end{pmatrix},\n",
        "max\\begin{pmatrix}\n",
        "0.761   \\\\\n",
        "-0.999  \\\\\n",
        "0.964   \\\\\n",
        "-0.995  \\\\\n",
        "0\n",
        "\\end{pmatrix}\\right] = [0.964,0.999,0.964]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZdqJSNZKEBq",
        "outputId": "9447b188-74e6-4d3d-a84a-43ca1b3c577e"
      },
      "source": [
        "# puedes comprobar tus calculos con algunas lineas de c√≥digo python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "U = np.array([[-1, 1, 0],\n",
        "              [1,1,0],\n",
        "              [0,0,-1],\n",
        "              [1,-1,-1],\n",
        "              [-1,-1,1],\n",
        "              [1,0,-1]])\n",
        "X = np.array([[2,2,0,-2,0,1],\n",
        "              [0,-2,0,1,-2,1],\n",
        "              [0,1,-2,1,1,0],\n",
        "              [-2,-2,1,0,-1,1],\n",
        "              [1,0,-1,1,1,1]])\n",
        "P = np.tanh(np.matmul(X,U))\n",
        "\n",
        "C = np.max(P, axis = 0)\n",
        "print(\"P= \",P)\n",
        "print(\"C= \",C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P=  [[-0.76159416  0.99998771  0.76159416]\n",
            " [ 0.96402758 -0.76159416 -0.9993293 ]\n",
            " [ 0.76159416 -0.76159416  0.96402758]\n",
            " [ 0.96402758 -0.99505475 -0.99505475]\n",
            " [ 0.         -0.76159416  0.        ]]\n",
            "C=  [0.96402758 0.99998771 0.96402758]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj1V_sAzZCHY"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "### Pregunta 4 (1 pt)\n",
        "Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
        "\n",
        "Tenemos una red recurrente *Elman* definidad como: \n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
        "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "donde\n",
        "\\begin{equation}\n",
        "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
        "\\end{equation}\n",
        "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
        "\n",
        "Sea\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_0 &= [0,0,0]\\\\\n",
        "W^x &= \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} \\\\\n",
        "W^s &= \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix} \\\\\n",
        "\\vec{b} &= [0, 0, 0] \\\\\n",
        "g(x) &= ReLu(x) = max(0, x)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M7sqIQV-Q3a"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{s}_1 = ReLu(\\vec{s_0}W^s + \\vec{x_0}W^x+\\vec{b}) = ReLu([0,0,0]\\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}   + [1,0]\\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix}+[0,0,0]) = ReLu([0,0,1]) = [0,0,1]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{s}_2 = ReLu(\\vec{s_1}W^s + \\vec{x_2}W^x+\\vec{b}) = ReLu([0,0,1]\\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}   + [-1,1]\\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix}+[0,0,0])  = ReLu([2,0,0]) = [2,0,0]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{s}_3 = ReLu(\\vec{s_2}W^s + \\vec{x_3}W^x+\\vec{b}) = ReLu([2,0,0]\\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}   + [1,1]\\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix}+[0,0,0])  = ReLu([3,-1,1]) = [3,0,1]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{y}_1 = [0,0,1]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{y}_2 = [2,0,0]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\vec{y}_3 = [3,0,1]\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4rAT6ELxRZW"
      },
      "source": [
        "### Pregunta 5 (0.5 pts)\n",
        "¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n",
        "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AXbQSgA_t8"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Las RNN y CNN pertenencen a arquitecturas de Deep Learning, que tienen como base las redes neuronales, los cuales son mucho m√°s robustos que los modelos cl√°sicos, como HMMs, MEMMs y CRFs. \n",
        "\n",
        "Las RNNs y CNNs logran aprender gracias al algoritmo de *backpropagation*, el cual calcula los gradientes despu√©s de cada pasada *forward* en la red y permite ir minimizando una funci√≥n de p√©rdida previamente definida. A diferencia de, por ejemplo, las HMMs, que pertenecen a modelos generativos y modelan la probabilidad conjunta en base a conteos y posteriormente se decodifica con *Viterbi*, en otras palabras las HMMs \"aprenden\" en base a conteos y mediante el algoritmo de programaci√≥n din√°mica se busca la secuencia m√°s probable dado un input. \n",
        "\n",
        "Por otro lado, los MEMMs y CRFs son modelos discriminativos, y pertenecen a modelos linales, los cuales \"aprenden\" gracias a descenso de gradiente y se busca obtener los par√°metros que maximizan la log-probabilidad de los datos deentrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQIuO8axTUa"
      },
      "source": [
        "# Redes neuronales con PyTorch\n",
        "### Pregunta 6 (2 pts)\n",
        "En esta parte van a tener que implementar una red neuronal Feed Forward. Adem√°s, deber√°n entrenar el modelo usando uno de los datasets de TorchText. En la secci√≥n de la respuesta hay un esqueleto de lo que deben hacer, deber√°n completar los metodos del modelo e implementar la parte asociada al entrenamiento. Como les mencionamos en la [Auxiliar 3](https://www.youtube.com/watch?v=r1pt9v_vuy8&list=PLwITEwTGVF6jXRu6l55W4OedqDiLgU9OM&index=2&t=5s), el proceso de entrenamiento es bastante est√°ndar, as√≠ que se pueden guiar en gran medida por los ejemplos que ah√≠ mostramos y los que vamos a ver en las pr√≥ximas auxiliares.\n",
        "\n",
        "#### 6.1 Capa Convolucional (Opcional)\n",
        "Agregue a la arquitectura una capa convolucional. Para esto puede registrar el parametro $U$ en la red y realizar el computo de la convoluci√≥n en el metodo forward de la red, o puede usar la clase [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) de `torch`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVKEaQXZ3eGl"
      },
      "source": [
        "%%capture\n",
        "# Nos aseguramos que torchtext este en la ultima version\n",
        "!pip install --upgrade torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-wrzFO5mCC"
      },
      "source": [
        "# Trabajaremos con el dataset AG_NEWS de torchtext\n",
        "# https://pytorch.org/text/stable/datasets.html#ag-news\n",
        "import os\n",
        "import torch # torchtext 0.10.0\n",
        "from random import choice, sample\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "train_dataset, test_dataset = AG_NEWS(root=\"data\")\n",
        "\n",
        "train_list = list(train_dataset)\n",
        "test_list = list(test_dataset)\n",
        "\n",
        "# Informacion relevante del dataset\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator([tokenizer(x[1]) for x in train_list], specials=['<unk>','<pad>'], special_first=True) # Creamos el vocabulario\n",
        "vocab.set_default_index(vocab['<unk>']) # Definimos el caracter <unk> para palabras que no est√©n en el vocabulario\n",
        "num_classes = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA7BJ89mHPwQ",
        "outputId": "4325a7a2-c144-475d-dc3d-b96431b1f282"
      },
      "source": [
        "print('Algunos tokens:', sample(vocab.get_itos(), 5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Algunos tokens: ['once-fledgling', 'traffickers', 'matera', 'sofia', 'yakubu']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXngUm9HxKvA"
      },
      "source": [
        "# De aca para abajo viene su respuesta, completen las funciones en la red\n",
        "# y luego entrenen el modelo y evaluenlo usando los dataset que acaban de\n",
        "# cargar\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=4, use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "        super().__init__()\n",
        "        # Aca deben registrar sus parametros. A lo menos necesitan\n",
        "        # una capa de embedding y un MLP basico (una capa lineal + softmax)\n",
        "        self.use_cnn = use_cnn\n",
        "        if self.use_cnn:\n",
        "            self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            embed_dim,\n",
        "            padding_idx=vocab[\"<pad>\"]\n",
        "            )\n",
        "            self.conv = nn.Conv1d( # Capa convolucional\n",
        "                in_channels=1,\n",
        "                out_channels=num_classes,\n",
        "                kernel_size=cnn_kernel_size*embed_dim,\n",
        "                stride=embed_dim\n",
        "            )\n",
        "        else:\n",
        "            self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode = \"max\") # capa de embedding\n",
        "            self.fc = nn.Linear(embed_dim, num_classes) # capa MLP\n",
        "    def forward(self, text): # Reemplacen el *args por sus argumentos\n",
        "        # Ac√° debe programar la pasada hacia adelante\n",
        "        # Embeddeg -> B,N,E\n",
        "        embedded = self.embedding(text)\n",
        "        if self.use_cnn:\n",
        "            embedded = embedded.view(embedded.shape[0], 1, -1) # flatten a las frases\n",
        "            # z -> B, num_class, N\n",
        "            z = F.relu(self.conv(embedded))\n",
        "\n",
        "            return z.max(dim=-1).values\n",
        "\n",
        "        # B, num_class\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPHyqgId96ra"
      },
      "source": [
        "# El resto de su respuesta. Ac√° deben programar el entrenamiento de la red\n",
        "\n",
        "import sys\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def generate_batch(batch):\n",
        "    # hint: si definen la capa de embedding del modelo usando nn.EmbeddingBag,\n",
        "    # les puede ayudar computar offsets para cada elemento del batch\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label - 1) # Guardamos el label\n",
        "        processed_text = torch.tensor([vocab[w] for w in tokenizer(_text)]) # Creamos el tensor con los indices del batch\n",
        "        text_list.append(processed_text)\n",
        "    return (\n",
        "        pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"]), # Hacemos padding\n",
        "        torch.tensor(label_list) # Almacenamos los labels en un tensor\n",
        "    )\n",
        "\n",
        "# Intenten superar un Accuracy de 90% en el conjunto de test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irOHmPiB40WW"
      },
      "source": [
        "# Se definen las funciones para entrenar\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_func(sub_train_, model):\n",
        "    # Train the model\n",
        "    train_loss ,train_acc = 0, 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                        collate_fn=generate_batch)\n",
        "    for text, cls in data:\n",
        "        optimizer.zero_grad()\n",
        "        text, cls = text.to(device), cls.to(device)\n",
        "        output = model(text)\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
        "\n",
        "def test(data_, model):\n",
        "    loss, acc = 0, 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, cls in data:\n",
        "        text, cls = text.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "    return loss / len(data_), acc / len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_hQn7aTI5pd"
      },
      "source": [
        "Primero, entrenaremos el modelo sin capa convolucional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQl3pm6FI_ZK",
        "outputId": "e20b5822-1ba0-41b8-dd7b-1bd762032c0a"
      },
      "source": [
        "# Entrenamos el modelo\n",
        "\n",
        "import time, random\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Setear seed para hacer resultados comparables\n",
        "torch.manual_seed(8888)\n",
        "random.seed(8888)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "N_EPOCHS = 15\n",
        "BATCH_SIZE = 64\n",
        "EMBED_DIM = 32\n",
        "LEARN_RATE = 2.0\n",
        "VOCAB_SIZE = len(vocab)\n",
        "NUM_CLASSES = num_classes\n",
        "USE_CNN = False # No utilizar capa covolucional\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "train_len = int(len(train_list) * 0.85) \n",
        "sub_train_, sub_valid_ = random_split(train_list, [train_len, len(train_list) - train_len]) # Creamos conjunto de Train y Validation.\n",
        "\n",
        "model_fc = CNNClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASSES, use_cnn=USE_CNN).to(device) # Definimos el modelo\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) # funci√≥n de p√©rdida \n",
        "optimizer = torch.optim.SGD(model_fc.parameters(), lr=LEARN_RATE) #optimizador\n",
        "\n",
        "print(\"Train data: {}\\nValidation data: {}\\nTest Data: {}\".format(len(sub_train_), len(sub_valid_), len(test_dataset)))\n",
        "print(f\"\\nEntrenando modelo {model_fc}\\n\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_, model_fc)\n",
        "    valid_loss, valid_acc = test(sub_valid_, model_fc)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Train data: 102000\n",
            "Validation data: 18000\n",
            "Test Data: 7600\n",
            "\n",
            "Entrenando modelo CNNClassifier(\n",
            "  (embedding): EmbeddingBag(95812, 32, mode=max)\n",
            "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
            ")\n",
            "\n",
            "Epoch: 1  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.1414(train)\t|\tAcc: 71.5%(train)\n",
            "\tLoss: 0.0004(valid)\t|\tAcc: 86.0%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0368(train)\t|\tAcc: 83.6%(train)\n",
            "\tLoss: 0.0003(valid)\t|\tAcc: 85.7%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0233(train)\t|\tAcc: 86.9%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 87.9%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0173(train)\t|\tAcc: 89.2%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 88.0%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0123(train)\t|\tAcc: 91.3%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 80.6%(valid)\n",
            "Epoch: 6  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0093(train)\t|\tAcc: 92.6%(train)\n",
            "\tLoss: 0.0004(valid)\t|\tAcc: 83.0%(valid)\n",
            "Epoch: 7  | time in 0 minutes, 9 seconds\n",
            "\tLoss: 0.0069(train)\t|\tAcc: 94.0%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 84.7%(valid)\n",
            "Epoch: 8  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0048(train)\t|\tAcc: 95.2%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 84.5%(valid)\n",
            "Epoch: 9  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0037(train)\t|\tAcc: 95.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 86.0%(valid)\n",
            "Epoch: 10  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 96.7%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 84.9%(valid)\n",
            "Epoch: 11  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0021(train)\t|\tAcc: 97.4%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 87.0%(valid)\n",
            "Epoch: 12  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0016(train)\t|\tAcc: 97.8%(train)\n",
            "\tLoss: 0.0003(valid)\t|\tAcc: 87.3%(valid)\n",
            "Epoch: 13  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0012(train)\t|\tAcc: 98.3%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 86.9%(valid)\n",
            "Epoch: 14  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0009(train)\t|\tAcc: 98.6%(train)\n",
            "\tLoss: 0.0003(valid)\t|\tAcc: 87.2%(valid)\n",
            "Epoch: 15  | time in 0 minutes, 8 seconds\n",
            "\tLoss: 0.0008(train)\t|\tAcc: 98.9%(train)\n",
            "\tLoss: 0.0002(valid)\t|\tAcc: 87.5%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHJqcOhz5kZV",
        "outputId": "fee9e9a0-4a27-4531-f2b0-adee25af90e4"
      },
      "source": [
        "print('Simple fully connected model:')\n",
        "test_loss, test_acc = test(test_list, model_fc) # Evaluamos el conjunto de testing\n",
        "print(f'Loss: {test_loss:.4f}(test)\\nAccuracy: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple fully connected model:\n",
            "Loss: 0.0004(test)\n",
            "Accuracy: 86.5%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUqLLJLgKGJy"
      },
      "source": [
        "Ahora entrenamos el modelo con capa convolucional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOfHkt365szs",
        "outputId": "e35241ee-3a0d-4ce1-cf8b-907b847ea967"
      },
      "source": [
        "# Entrenamos el modelo\n",
        "\n",
        "import time, random\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# Setear seed para hacer resultados comparables\n",
        "torch.manual_seed(8888)\n",
        "random.seed(8888)\n",
        "\n",
        "# Hiperpar√°metros\n",
        "N_EPOCHS = 20\n",
        "BATCH_SIZE = 128\n",
        "EMBED_DIM = 32\n",
        "LEARN_RATE = 3.0\n",
        "VOCAB_SIZE = len(vocab)\n",
        "NUM_CLASSES = num_classes\n",
        "USE_CNN = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "train_len = int(len(train_list) * 0.85) \n",
        "sub_train_, sub_valid_ = random_split(train_list, [train_len, len(train_list) - train_len]) # Creamos conjunto de Train y Validation.\n",
        "\n",
        "model = CNNClassifier(VOCAB_SIZE, EMBED_DIM, NUM_CLASSES, use_cnn=USE_CNN).to(device) # Definimos el modelo\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) # funci√≥n de p√©rdida \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE) #optimizador\n",
        "\n",
        "print(\"Train data: {}\\nValidation data: {}\\nTest Data: {}\".format(len(sub_train_), len(sub_valid_), len(test_dataset)))\n",
        "print(f\"\\nEntrenando modelo {model}\\n\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_, model)\n",
        "    valid_loss, valid_acc = test(sub_valid_, model)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Train data: 102000\n",
            "Validation data: 18000\n",
            "Test Data: 7600\n",
            "\n",
            "Entrenando modelo CNNClassifier(\n",
            "  (embedding): Embedding(95812, 32, padding_idx=1)\n",
            "  (conv): Conv1d(1, 4, kernel_size=(96,), stride=(32,))\n",
            ")\n",
            "\n",
            "Epoch: 1  | time in 0 minutes, 11 seconds\n",
            "\tLoss: 0.0124(train)\t|\tAcc: 58.7%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 69.4%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0073(train)\t|\tAcc: 71.7%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 74.8%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0062(train)\t|\tAcc: 75.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 77.9%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0056(train)\t|\tAcc: 78.0%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 80.0%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0052(train)\t|\tAcc: 79.5%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 80.2%(valid)\n",
            "Epoch: 6  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0048(train)\t|\tAcc: 80.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 76.1%(valid)\n",
            "Epoch: 7  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0046(train)\t|\tAcc: 81.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 82.6%(valid)\n",
            "Epoch: 8  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0044(train)\t|\tAcc: 82.6%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 82.7%(valid)\n",
            "Epoch: 9  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0042(train)\t|\tAcc: 83.1%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 83.6%(valid)\n",
            "Epoch: 10  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0040(train)\t|\tAcc: 83.8%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 81.4%(valid)\n",
            "Epoch: 11  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0039(train)\t|\tAcc: 84.3%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 83.5%(valid)\n",
            "Epoch: 12  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0038(train)\t|\tAcc: 84.7%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 84.3%(valid)\n",
            "Epoch: 13  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0036(train)\t|\tAcc: 85.4%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 84.7%(valid)\n",
            "Epoch: 14  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0035(train)\t|\tAcc: 85.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 84.5%(valid)\n",
            "Epoch: 15  | time in 0 minutes, 10 seconds\n",
            "\tLoss: 0.0034(train)\t|\tAcc: 86.2%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 84.5%(valid)\n",
            "Epoch: 16  | time in 0 minutes, 11 seconds\n",
            "\tLoss: 0.0033(train)\t|\tAcc: 86.5%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 84.9%(valid)\n",
            "Epoch: 17  | time in 0 minutes, 11 seconds\n",
            "\tLoss: 0.0032(train)\t|\tAcc: 87.0%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 84.7%(valid)\n",
            "Epoch: 18  | time in 0 minutes, 11 seconds\n",
            "\tLoss: 0.0031(train)\t|\tAcc: 87.2%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 85.0%(valid)\n",
            "Epoch: 19  | time in 0 minutes, 11 seconds\n",
            "\tLoss: 0.0030(train)\t|\tAcc: 87.5%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 82.9%(valid)\n",
            "Epoch: 20  | time in 0 minutes, 11 seconds\n",
            "\tLoss: 0.0029(train)\t|\tAcc: 87.9%(train)\n",
            "\tLoss: 0.0001(valid)\t|\tAcc: 85.5%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_MbRbW5JGPe",
        "outputId": "b646b30d-5e37-440f-e757-b6f1e79a9c33"
      },
      "source": [
        "print('Convolutional model: ')\n",
        "test_loss, test_acc = test(test_list, model) # Evaluamos el conjunto de testing\n",
        "print(f'Loss: {test_loss:.4f}(test)\\nAccuracy: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convolutional model: \n",
            "Loss: 0.0001(test)\n",
            "Accuracy: 85.2%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}